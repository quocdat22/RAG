Phần 1: Định nghĩa Cốt lõi và Bối cảnh Lịch sử

Học sâu (Deep Learning) là một tập hợp con chuyên biệt của học máy (Machine Learning), mà bản thân học máy là một nhánh của trí tuệ nhân tạo (AI). Nó dựa trên các mạng nơ-ron nhân tạo, đặc biệt là những mạng có nhiều lớp giữa đầu vào và đầu ra, được gọi là mạng nơ-ron "sâu". Độ "sâu" ám chỉ số lượng lớp mà dữ liệu được biến đổi qua. Một cột mốc lịch sử quan trọng là thành công của AlexNet vào năm 2012, một mạng nơ-ron tích chập sâu, đã cải thiện đáng kể độ chính xác phân loại ảnh trong thử thách ImageNet, thúc đẩy việc áp dụng rộng rãi.

Phần 2: Kiến trúc Cơ bản và Ứng dụng Chính của Chúng

Phần này phác thảo ba kiến trúc mạng nơ-ron then chốt.

2.1 Mạng Nơ-ron Tích chập (CNNs): CNNs được thiết kế chủ yếu để xử lý dữ liệu dạng lưới như hình ảnh. Thành phần cốt lõi của chúng là các lớp tích chập, sử dụng bộ lọc để phát hiện các mẫu hình theo thứ bậc không gian (ví dụ: cạnh, kết cấu, vật thể). Các lớp gộp (Pooling) giảm kích thước không gian. Ứng dụng chủ đạo bao gồm nhận dạng hình ảnh, phát hiện đối tượng và phân tích hình ảnh y tế.

2.2 Mạng Nơ-ron Hồi quy (RNNs): RNNs được thiết kế cho dữ liệu tuần tự. Chúng có một trạng thái nội bộ (trạng thái ẩn) nắm bắt thông tin về các phần tử trước đó trong chuỗi. Một vấn đề phổ biến với RNN cơ bản là vấn đề tiêu biến/bùng nổ gradient, gây khó khăn cho việc học các phụ thuộc tầm xa. Chúng truyền thống được sử dụng trong dự báo chuỗi thời gian và các tác vụ xử lý ngôn ngữ tự nhiên (NLP) thời kỳ đầu.

2.3 Kiến trúc Transformer: Được giới thiệu trong bài báo năm 2017 "Attention Is All You Need", kiến trúc Transformer hoàn toàn dựa trên cơ chế tự chú ý (self-attention) để đánh trọng số tầm quan trọng của các phần khác nhau trong dữ liệu đầu vào, bất kể khoảng cách giữa chúng. Điều này cho phép xử lý song song vượt trội và xử lý ngữ cảnh tầm xa so với RNNs. Transformer là kiến trúc nền tảng cho các mô hình ngôn ngữ lớn hiện đại (LLMs) như GPT-4, BERT và T5, cách mạng hóa lĩnh vực NLP.

Phần 3: Các Khái niệm Quan trọng và Thách thức Huấn luyện

3.1 Quy trình Huấn luyện: Huấn luyện một mô hình học sâu liên quan đến việc điều chỉnh các tham số (trọng số và độ lệch) của nó để cực tiểu hóa một hàm mất mát (loss function). Điều này thường được thực hiện bằng một thuật toán tối ưu hóa như Gradient Descent Ngẫu nhiên (SGD) hoặc các biến thể của nó (ví dụ: Adam). Lan truyền ngược (Backpropagation) là thuật toán được sử dụng để tính gradient của hàm mất mát đối với từng tham số, cho phép bộ tối ưu hóa thực hiện các điều chỉnh.

3.2 Vấn đề Quá khớp (Overfitting) và Kỹ thuật Chuẩn hóa: Quá khớp xảy ra khi một mô hình học tập dữ liệu huấn luyện quá tốt, bao gồm cả nhiễu của nó, và không thể tổng quát hóa cho dữ liệu chưa từng thấy. Các kỹ thuật chuẩn hóa phổ biến để chống lại điều này bao gồm:
- Dropout: Loại bỏ ngẫu nhiên các đơn vị (và các kết nối của chúng) trong quá trình huấn luyện.
- Chuẩn hóa L1/L2: Thêm một lượng phạt vào hàm mất mát dựa trên độ lớn của trọng số.
- Dừng sớm (Early Stopping): Dừng huấn luyện khi hiệu suất trên một tập kiểm định (validation set) ngừng cải thiện.
- Tăng cường Dữ liệu (Data Augmentation): Tăng nhân tạo kích thước và độ đa dạng của tập huấn luyện (ví dụ: xoay hình ảnh).

3.3 Sự Phụ thuộc vào Phần cứng: Các phép toán ma trận cốt lõi trong học sâu đòi hỏi tính toán cường độ cao. Bộ xử lý đồ họa (GPU) và gần đây hơn là Bộ xử lý Tensor (TPU) là thiết yếu nhờ kiến trúc song song lớn của chúng, giúp tăng tốc thời gian huấn luyện từ nhiều tháng xuống còn vài ngày hoặc vài giờ.

Phần 4: Mẫu Hỏi đáp để Kiểm tra Truy xuất

H: Đâu là cải tiến kiến trúc chính phân biệt Transformer với RNN?
Đ: Transformer thay thế hoàn toàn các kết nối hồi quy bằng cơ chế tự chú ý (self-attention), cho phép nó xử lý tất cả các phần tử của một chuỗi song song và mô hình hóa các phụ thuộc bất kể khoảng cách một cách hiệu quả hơn.

H: Hãy kể tên hai kỹ thuật được sử dụng để ngăn chặn hiện tượng quá khớp trong mạng nơ-ron sâu.
Đ: Hai kỹ thuật phổ biến là Dropout và Chuẩn hóa L2.

H: Tại sao GPU đặc biệt phù hợp cho việc huấn luyện học sâu?
Đ: GPU sở hữu hàng nghìn lõi nhỏ hơn được tối ưu hóa cho tính toán song song, khiến chúng có hiệu suất cao cho các phép toán ma trận và vectơ chi phối quá trình huấn luyện mạng nơ-ron.

H: Kiến trúc học sâu nào phù hợp nhất để phân tích một tập dữ liệu chuỗi thời gian về giá cổ phiếu hàng ngày, và tại sao?
Đ: Mặc dù Transformer ngày càng được sử dụng, Mạng Nơ-ron Hồi quy (RNNs) hoặc các biến thể nâng cao của chúng như LSTMs vẫn phù hợp một cách kinh điển vì chúng được thiết kế rõ ràng để mô hình hóa các phụ thuộc tuần tự/theo thời gian trong dữ liệu.

H: Sự kiện quan trọng nào vào năm 2012 thường được coi là chất xúc tác cho sự bùng nổ của học sâu hiện đại?
Đ: Chiến thắng của mô hình CNN AlexNet trong Thử thách Nhận dạng Hình ảnh Quy mô Lớn ImageNet (ILSVRC), mô hình này đã vượt trội đáng kể so với tất cả các phương pháp thị giác máy tính truyền thống.