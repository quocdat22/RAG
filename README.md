# RAG System for Analyst

A production-ready Retrieval-Augmented Generation (RAG) system built with LlamaIndex and Chroma, designed for data analyst use cases.

## ğŸ—ï¸ Architecture

This system follows a 7-layer architecture optimized for analytical workloads:

1. **Data Ingestion Layer** - Document loaders and chunking strategies
2. **Embedding & Storage Layer** - Vector embeddings with Chroma
3. **Query Processing Layer** - Intelligent query routing and transformation
4. **LlamaIndex Core** - Orchestration and context management
5. **Analysis & Output Layer** - Multi-format responses (text, charts, tables)
6. **Monitoring & Optimization** - Performance tracking and quality metrics
7. **Caching & Performance** - Multi-level caching for speed

## ğŸš€ Features

- **Multi-format Document Support**: PDF, TXT, CSV, DOCX, XLSX
- **Intelligent Retrieval**: Hybrid search with vector + keyword matching
- **Re-ranking**: Cohere rerank-v3.5 for improved relevance
- **Incremental Indexing**: Only process new documents
- **Streamlit UI**: Easy-to-use web interface
- **Caching**: Optional Redis caching for performance
- **Modular Design**: Clean separation of concerns

## ğŸ“‹ Prerequisites

- Python 3.10 or higher
- UV package manager
- OpenAI API key (GitHub Models or OpenAI)
- Cohere API key (for reranking)
- Redis (optional, for caching)

## ğŸ”§ Installation

### 1. Clone and Setup

```bash
cd c:\Users\dat\Projects\RAG
```

### 2. Install Dependencies with UV

```bash
# Install production dependencies
uv sync

# Install with development dependencies
uv sync --extra dev
```

### 3. Configure Environment

```bash
# Copy environment template
cp .env.example .env

# Edit .env with your API keys
notepad .env
```

Required environment variables:
- `OPENAI_API_KEY`: Your OpenAI or GitHub Models API key
- `COHERE_API_KEY`: Your Cohere API key for reranking

## ğŸ¯ Quick Start

### 1. Run the Streamlit UI

```bash
uv run streamlit run ui/app.py
```

### 2. Upload Documents

- Navigate to http://localhost:8501
- Upload PDF, TXT, CSV, or other supported files
- Wait for indexing to complete

### 3. Query Your Data

- Enter your question in the query box
- Get AI-powered answers with source citations
- View retrieved documents and metadata

## ğŸ“ Project Structure

```
RAG/
â”œâ”€â”€ config/              # Configuration management
â”‚   â”œâ”€â”€ settings.py      # Centralized settings
â”‚   â””â”€â”€ prompts.py       # LLM prompt templates
â”œâ”€â”€ src/                 # Main application code
â”‚   â”œâ”€â”€ ingestion/       # Document loading & chunking
â”‚   â”œâ”€â”€ embedding/       # Vector embeddings
â”‚   â”œâ”€â”€ retrieval/       # Query processing & retrieval
â”‚   â”œâ”€â”€ generation/      # LLM integration
â”‚   â””â”€â”€ core/            # Utilities (cache, logging, etc.)
â”œâ”€â”€ ui/                  # Streamlit interface
â”œâ”€â”€ tests/               # Test suite
â”œâ”€â”€ data/                # Data storage
â”‚   â”œâ”€â”€ documents/       # Uploaded files
â”‚   â””â”€â”€ vector_db/       # Chroma database
â””â”€â”€ notebooks/           # Jupyter notebooks
```

## ğŸ§ª Testing

```bash
# Run all tests
uv run pytest

# Run with coverage
uv run pytest --cov=src --cov-report=html

# Run specific test file
uv run pytest tests/unit/test_ingestion.py
```

## ğŸ› ï¸ Development

### Code Quality

```bash
# Format code
uv run black src/ ui/ tests/

# Lint code
uv run ruff check src/ ui/ tests/

# Type checking
uv run mypy src/
```

### Adding Dependencies

```bash
# Add production dependency
uv add package-name

# Add development dependency
uv add --dev package-name
```

## ğŸ“Š Usage Examples

### Simple Q&A

```python
from src.retrieval.query_processor import QueryProcessor
from src.generation.llm_client import LLMClient

# Initialize
processor = QueryProcessor()
llm = LLMClient()

# Query
result = processor.process_query("What is the revenue for Q3 2024?")
print(result.answer)
```

### Advanced Analysis

```python
# Complex analytical query
result = processor.process_query(
    "Analyze revenue trends over the last 3 years and suggest strategies"
)
print(result.answer)
print(result.sources)
```

## âš™ï¸ Configuration

Key configuration options in `.env`:

| Variable | Description | Default |
|----------|-------------|---------|
| `CHUNK_SIZE` | Token size for chunks | 512 |
| `CHUNK_OVERLAP` | Overlap between chunks | 50 |
| `RETRIEVAL_TOP_K` | Number of docs to retrieve | 5 |
| `ENABLE_RERANKING` | Use Cohere reranking | true |
| `ENABLE_CACHE` | Enable Redis caching | false |

## ğŸ› Troubleshooting

### Common Issues

**Issue**: `ModuleNotFoundError: No module named 'chromadb'`
```bash
uv sync --reinstall
```

**Issue**: Chroma database locked
```bash
# Delete and recreate vector DB
rm -rf data/vector_db/*
# Re-index documents
```

**Issue**: Out of memory during indexing
```bash
# Reduce chunk size in .env
CHUNK_SIZE=256
```

## ğŸ“ˆ Performance

- **Query Latency**: < 3s (P95)
- **Cache Hit Rate**: 60%+ (with Redis)
- **Retrieval Precision**: 85%+
- **Supported Scale**: 100K-1M documents

## ğŸ” Security

- API keys stored in `.env` (never committed)
- Document data stays local
- Optional Redis authentication
- HTTPS recommended for production

## ğŸ“ License

MIT License

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and linting
5. Submit a pull request

## ğŸ“ Support

For issues and questions:
- Create a GitHub issue
- Check the documentation
- Review conversation history

## ğŸ—ºï¸ Roadmap

### Phase 1 (MVP) âœ“
- Basic Q&A functionality
- PDF, TXT, CSV support
- Streamlit UI
- Local deployment

### Phase 2 (Planned)
- Chart/graph visualization
- Multi-step analysis
- Advanced caching
- Performance dashboard

### Phase 3 (Future)
- Multi-language support
- Real-time data streaming
- User authentication
- Cloud deployment
