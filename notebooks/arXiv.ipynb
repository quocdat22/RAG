{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28fd1120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé ƒêang g·ª≠i y√™u c·∫ßu ƒë·∫øn arXiv API...\n",
      "\n",
      "--- K·∫øt qu·∫£ Atom XML Th√¥ (Raw XML) ---\n",
      "<?xml version='1.0' encoding='UTF-8'?>\n",
      "<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n",
      "  <id>https://arxiv.org/api/S94iiWltEbprnrxZqgUpdO3X89s</id>\n",
      "  <title>arXiv Query: search_query=ti:Quantum OR all:Computing&amp;id_list=&amp;start=0&amp;max_results=5</title>\n",
      "  <updated>2025-12-16T05:48:13Z</updated>\n",
      "  <link href=\"https://arxiv.org/api/query?search_query=ti:Quantum+OR+all:Computing&amp;start=0&amp;max_results=5&amp;id_list=\" type=\"application/atom+xml\"/>\n",
      "  <opensearch:itemsPerPage>5</opensearch:itemsPerPage>\n",
      "  <opensearch:totalResults>546234</opensearch:totalResults>\n",
      "  <opensearch:startIndex>0</opensearch:startIndex>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2512.13692v1</id>\n",
      "    <title>Quantum oracles give an advantage for identifying classical counterfactuals</title>\n",
      "    <updated>2025-12-15T18:59:58Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2512.13692v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2512.13692v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We show that quantum oracles provide an advantage over classical oracles for answering classical counterfactual questions in causal models, or equivalently, for identifying unknown causal parameters such as distributions over functional dependences. In structural causal models with discrete classical variables, observational data and even ideal interventions generally fail to answer all counterfactual questions, since different causal parameters can reproduce the same observational and interventional data while disagreeing on counterfactuals. Using a simple binary example, we demonstrate that if the classical variables of interest are encoded in quantum systems and the causal dependence among them is encoded in a quantum oracle, coherently querying the oracle enables the identification of all causal parameters -- hence all classical counterfactuals. We generalize this to arbitrary finite cardinalities and prove that coherent probing 1) allows the identification of all two-way joint counterfactuals p(Y_x=y, Y_{x'}=y'), which is not possible with any number of queries to a classical oracle, and 2) provides tighter bounds on higher-order multi-way counterfactuals than with a classical oracle. This work can also be viewed as an extension to traditional quantum oracle problems such as Deutsch--Jozsa to identifying more causal parameters beyond just, e.g., whether a function is constant or balanced. Finally, we raise the question of whether this quantum advantage relies on uniquely non-classical features like contextuality. We provide some evidence against this by showing that in the binary case, oracles in some classically-explainable theories like Spekkens' toy theory also give rise to a counterfactual identifiability advantage over strictly classical oracles.</summary>\n",
      "    <category term=\"quant-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2025-12-15T18:59:58Z</published>\n",
      "    <arxiv:comment>5+4 pages. Comments welcome!</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"quant-ph\"/>\n",
      "    <author>\n",
      "      <name>Ciar√°n M. Gilligan-Lee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Y√¨l√® Yƒ´ng</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jonathan Richens</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>David Schmid</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2512.13689v1</id>\n",
      "    <title>LitePT: Lighter Yet Stronger Point Transformer</title>\n",
      "    <updated>2025-12-15T18:59:57Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2512.13689v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2512.13689v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\\times$ fewer parameters, runs $2\\times$ faster, and uses $2\\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2025-12-15T18:59:57Z</published>\n",
      "    <arxiv:comment>Project page: https://litept.github.io/</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Yuanwen Yue</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Damien Robert</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jianyuan Wang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Sunghwan Hong</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Jan Dirk Wegner</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Christian Rupprecht</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Konrad Schindler</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2512.13687v1</id>\n",
      "    <title>Towards Scalable Pre-training of Visual Tokenizers for Generation</title>\n",
      "    <updated>2025-12-15T18:59:54Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2512.13687v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2512.13687v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2025-12-15T18:59:54Z</published>\n",
      "    <arxiv:comment>Our pre-trained models are available at https://github.com/MiniMax-AI/VTP</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Jingfeng Yao</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yuda Song</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yucong Zhou</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Xinggang Wang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2512.13684v1</id>\n",
      "    <title>Recurrent Video Masked Autoencoders</title>\n",
      "    <updated>2025-12-15T18:59:48Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2512.13684v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2512.13684v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2025-12-15T18:59:48Z</published>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Daniel Zoran</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Nikhil Parthasarathy</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yi Yang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Drew A Hudson</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Joao Carreira</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Andrew Zisserman</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "  <entry>\n",
      "    <id>http://arxiv.org/abs/2512.13680v1</id>\n",
      "    <title>LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction</title>\n",
      "    <updated>2025-12-15T18:59:04Z</updated>\n",
      "    <link href=\"https://arxiv.org/abs/2512.13680v1\" rel=\"alternate\" type=\"text/html\"/>\n",
      "    <link href=\"https://arxiv.org/pdf/2512.13680v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n",
      "    <summary>Recent feed-forward reconstruction models like VGGT and $œÄ^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\\href{https://neu-vi.github.io/LASER/}{\\texttt{https://neu-vi.github.io/LASER/}}$</summary>\n",
      "    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n",
      "    <published>2025-12-15T18:59:04Z</published>\n",
      "    <arxiv:comment>16 pages</arxiv:comment>\n",
      "    <arxiv:primary_category term=\"cs.CV\"/>\n",
      "    <author>\n",
      "      <name>Tianye Ding</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yiming Xie</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Yiqing Liang</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Moitreya Chatterjee</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Pedro Miraldo</name>\n",
      "    </author>\n",
      "    <author>\n",
      "      <name>Huaizu Jiang</name>\n",
      "    </author>\n",
      "  </entry>\n",
      "</feed>\n",
      "\n",
      "--------------------------------------\n",
      "\n",
      "--- Th√¥ng tin B√†i b√°o (ƒê√£ L·ªçc ƒê∆°n Gi·∫£n) ---\n",
      "‚úÖ B√†i b√°o #1:\n",
      "   - Ti√™u ƒë·ªÅ: Quantum oracles give an advantage for identifying classical counterfactuals\n",
      "   - ID arXiv: 2512.13692v1\n",
      "   - Link: https://arxiv.org/abs/2512.13692v1\n",
      "\n",
      "‚úÖ B√†i b√°o #2:\n",
      "   - Ti√™u ƒë·ªÅ: LitePT: Lighter Yet Stronger Point Transformer\n",
      "   - ID arXiv: 2512.13689v1\n",
      "   - Link: https://arxiv.org/abs/2512.13689v1\n",
      "\n",
      "‚úÖ B√†i b√°o #3:\n",
      "   - Ti√™u ƒë·ªÅ: Towards Scalable Pre-training of Visual Tokenizers for Generation\n",
      "   - ID arXiv: 2512.13687v1\n",
      "   - Link: https://arxiv.org/abs/2512.13687v1\n",
      "\n",
      "‚úÖ B√†i b√°o #4:\n",
      "   - Ti√™u ƒë·ªÅ: Recurrent Video Masked Autoencoders\n",
      "   - ID arXiv: 2512.13684v1\n",
      "   - Link: https://arxiv.org/abs/2512.13684v1\n",
      "\n",
      "‚úÖ B√†i b√°o #5:\n",
      "   - Ti√™u ƒë·ªÅ: LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction\n",
      "   - ID arXiv: 2512.13680v1\n",
      "   - Link: https://arxiv.org/abs/2512.13680v1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# 1. ƒê·ªãnh nghƒ©a URL c∆° s·ªü (Base URL) c·ªßa arXiv API\n",
    "ARXIV_BASE_URL = \"http://export.arxiv.org/api/query?\"\n",
    "\n",
    "# 2. ƒê·ªãnh nghƒ©a c√°c tham s·ªë truy v·∫•n (Query Parameters)\n",
    "# Truy v·∫•n n√†y t√¨m ki·∫øm 5 b√†i b√°o m·ªõi nh·∫•t trong danh m·ª•c cs.AI\n",
    "params = {\n",
    "    'search_query': 'ti:Quantum Computing', # T√¨m ki·∫øm trong danh m·ª•c (category) Khoa h·ªçc M√°y t√≠nh - Tr√≠ tu·ªá Nh√¢n t·∫°o\n",
    "    'start': 0,                   # B·∫Øt ƒë·∫ßu t·ª´ k·∫øt qu·∫£ th·ª© 0\n",
    "    'max_results': 5,             # Gi·ªõi h·∫°n 5 k·∫øt qu·∫£\n",
    "    'sortBy': 'submittedDate',    # S·∫Øp x·∫øp theo ng√†y n·ªôp\n",
    "    'sortOrder': 'descending'     # Th·ª© t·ª± gi·∫£m d·∫ßn (m·ªõi nh·∫•t tr∆∞·ªõc)\n",
    "}\n",
    "\n",
    "print(f\"üîé ƒêang g·ª≠i y√™u c·∫ßu ƒë·∫øn arXiv API...\")\n",
    "\n",
    "try:\n",
    "    # 3. G·ª≠i y√™u c·∫ßu GET\n",
    "    response = requests.get(ARXIV_BASE_URL, params=params)\n",
    "    response.raise_for_status() # N√©m ngo·∫°i l·ªá cho c√°c m√£ tr·∫°ng th√°i l·ªói HTTP\n",
    "\n",
    "    # 4. In k·∫øt qu·∫£ XML th√¥ (raw XML)\n",
    "    # K·∫øt qu·∫£ ƒë∆∞·ª£c tr·∫£ v·ªÅ d∆∞·ªõi ƒë·ªãnh d·∫°ng Atom XML\n",
    "    print(\"\\n--- K·∫øt qu·∫£ Atom XML Th√¥ (Raw XML) ---\")\n",
    "    print(response.text)\n",
    "    print(\"--------------------------------------\\n\")\n",
    "\n",
    "    # 5. Ph√¢n t√≠ch c√∫ ph√°p (Parsing) XML ƒë∆°n gi·∫£n (ƒê·ªÉ d·ªÖ ƒë·ªçc h∆°n)\n",
    "    # (L∆∞u √Ω: ƒê·ªÉ ph√¢n t√≠ch XML chuy√™n s√¢u, b·∫°n n√™n d√πng th∆∞ vi·ªán 'feedparser' ho·∫∑c 'xml.etree.ElementTree')\n",
    "    \n",
    "    print(\"--- Th√¥ng tin B√†i b√°o (ƒê√£ L·ªçc ƒê∆°n Gi·∫£n) ---\")\n",
    "    \n",
    "    # T√¨m ki·∫øm c√°c ti√™u ƒë·ªÅ (titles) v√† ID b√†i b√°o (IDs)\n",
    "    titles = [line.strip() for line in response.text.split('\\n') if '<title>' in line and 'arXiv' not in line]\n",
    "    ids = [line.strip() for line in response.text.split('\\n') if '<id>http://arxiv.org/abs/' in line]\n",
    "    \n",
    "    # In ra t·ª´ng b√†i b√°o\n",
    "    for i in range(min(len(titles), len(ids))):\n",
    "        # Tr√≠ch xu·∫•t ID b√†i b√°o\n",
    "        paper_id = ids[i].split('/abs/')[1].replace('</id>', '').strip()\n",
    "        # Tr√≠ch xu·∫•t ti√™u ƒë·ªÅ\n",
    "        title = titles[i].replace('<title>', '').replace('</title>', '').strip()\n",
    "        \n",
    "        print(f\"‚úÖ B√†i b√°o #{i+1}:\")\n",
    "        print(f\"   - Ti√™u ƒë·ªÅ: {title}\")\n",
    "        print(f\"   - ID arXiv: {paper_id}\")\n",
    "        print(f\"   - Link: https://arxiv.org/abs/{paper_id}\\n\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå ƒê√£ x·∫£y ra l·ªói khi k·∫øt n·ªëi: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
